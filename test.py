
import tensorflow as tf
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import os
import numpy as np

from sklearn.model_selection import train_test_split
from load_image import load_images
from process_image import process_image

def load_and_preprocess_images(folder_path):
    images = []
    for file_name in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file_name)
        if file_path.endswith(('.jpg', '.png', '.jpeg')):
            img = tf.keras.preprocessing.image.load_img(file_path, target_size=(512, 512))  # æ ¹æ®æ¨¡å‹è¾“å…¥è°ƒæ•´å°ºå¯¸
            img = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # å½’ä¸€åŒ–åˆ° [0, 1]
            images.append(img)
    return np.array(images)

# å®šä¹‰è®¡ç®— MSE å‡½æ•°
def calculate_mse(original, reconstructed):

    mse = tf.keras.losses.MeanSquaredError()
    return mse(original, reconstructed).numpy()
# ç¡®ä¿å›¾åƒæ˜¯ TensorFlow æ ¼å¼ä¸”å½’ä¸€åŒ–
def test_model_and_calculate_mse_difference(model, folder_original, folder_processed, save_path, num_samples=5):

    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    # åŠ è½½åŸå§‹å’Œå¤„ç†åçš„å›¾åƒ
    original_images = load_and_preprocess_images(folder_original)
    processed_images = load_and_preprocess_images(folder_processed)

    # å–æ ·æ•°é‡
    num_samples = min(num_samples, len(original_images), len(processed_images))
    print("Original Images Shape:", np.shape(original_images))
    print("Processed Images Shape:", np.shape(processed_images))

    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹ï¼Œåªå–ç¬¬ä¸€ä¸ªè¾“å‡º
    output_original = model.predict(original_images[:num_samples])
    output_processed = model.predict(processed_images[:num_samples])

    # å¦‚æœæ¨¡å‹è¿”å›å¤šä¸ªè¾“å‡ºï¼Œåªå–ç¬¬ä¸€ä¸ªè¾“å‡º
    reconstructed_original = output_original[0] if isinstance(output_original, list) else output_original
    reconstructed_processed = output_processed[0] if isinstance(output_processed, list) else output_processed

    print("Reconstructed Original Shape:", np.shape(reconstructed_original))
    print("Reconstructed Processed Shape:", np.shape(reconstructed_processed))

    # è®¡ç®— MSE
    mse_original = calculate_mse(original_images[:num_samples], reconstructed_original)
    mse_processed = calculate_mse(processed_images[:num_samples], reconstructed_processed)

    print(f"âœ… åŸå§‹å›¾åƒ MSE: {mse_original:.8f}")
    print(f"âœ… å¤„ç†åå›¾åƒ MSE: {mse_processed:.8f}")

    # ä¿å­˜å›¾åƒ
    plt.figure(figsize=(15, 10))
    for i in range(num_samples):
        # åŸå§‹å›¾åƒ
        plt.subplot(3, num_samples, i + 1)
        plt.imshow(original_images[i])
        plt.axis('off')
        plt.title("Original Image")
        plt.imsave(os.path.join(save_path, f"original_image_{i}.png"), original_images[i])

        # å¤„ç†åå›¾åƒ
        plt.subplot(3, num_samples, num_samples + i + 1)
        plt.imshow(processed_images[i])
        plt.axis('off')
        plt.title("Processed Image")
        plt.imsave(os.path.join(save_path, f"processed_image_{i}.png"), processed_images[i])

        # é‡å»ºå›¾åƒ
        plt.subplot(3, num_samples, 2 * num_samples + i + 1)
        plt.imshow(reconstructed_processed[i])
        plt.axis('off')
        plt.title("Reconstructed Processed")
        plt.imsave(os.path.join(save_path, f"reconstructed_image_{i}.png"), reconstructed_processed[i])

    plt.tight_layout()
    plt.show()

# è·¯å¾„é…ç½®
folder_original = "/home/gang/Programs/fish/test_image/gaussian_image_test"  # æ›¿æ¢ä¸ºåŸå§‹å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
folder_processed = "/home/gang/Programs/fish/test_image/gaussian_image_test"  # æ›¿æ¢ä¸ºå¤„ç†åå›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
save_path = "/home/gang/Programs/fish/test"  # æ›¿æ¢ä¸ºç»“æœä¿å­˜è·¯å¾„

# åŠ è½½æ¨¡å‹
model_path = "/home/gou/Programs/fish/result/model.h5"
model = load_model(model_path)
print("âœ… æ¨¡å‹å·²æˆåŠŸåŠ è½½ï¼")

# æµ‹è¯•æ¨¡å‹å¹¶è®¡ç®— MSE å·®å¼‚
test_model_and_calculate_mse_difference(model, folder_original, folder_processed, save_path)
"""
# 1ï¸âƒ£ åŠ è½½æ¨¡å‹
model_path = "/home/gou/Programs/fish/result/model.h5"
model = load_model(model_path, compile=False)  # ä¸ç¼–è¯‘ï¼Œå› ä¸ºæˆ‘ä»¬æ‰‹åŠ¨è®¡ç®— loss
print("âœ… Model has been loaded!")

# 2ï¸âƒ£ é¢„å¤„ç†æ•°æ®ï¼ˆç¡®ä¿æ•°æ®ç±»å‹æ˜¯ float32ï¼‰
images = load_images()  # åŠ è½½å›¾åƒ
images = process_image(images)  # é¢„å¤„ç†å›¾åƒ

x_train, x_val = train_test_split(images, test_size=0.2, random_state=42)
x_train = x_train.astype(np.float32)
x_val = x_val.astype(np.float32)

# 3ï¸âƒ£ å¤„ç†è®­ç»ƒé›† & æµ‹è¯•é›†
batch_size = 2  # ä½ è®­ç»ƒæ—¶çš„ batch_size
train_dataset = (
    tf.data.Dataset.from_tensor_slices(x_train)
    .map(lambda x: tf.cast(x, tf.float32))
    .batch(batch_size)
)
val_dataset = (
    tf.data.Dataset.from_tensor_slices(x_val)
    .map(lambda x: tf.cast(x, tf.float32))
    .batch(batch_size)
)

# 4ï¸âƒ£ è®¡ç®—æŸå¤±
def compute_losses(dataset, dataset_type="Train"):
    total_loss = tf.Variable(0.0, dtype=tf.float32)
    total_reco_loss = tf.Variable(0.0, dtype=tf.float32)
    total_kl_loss = tf.Variable(0.0, dtype=tf.float32)
    num_batches = tf.Variable(0, dtype=tf.int32)

    for x_batch in dataset:
        # é¢„æµ‹è¾“å‡º
        y_pred, z_mean, z_log_var = model(x_batch, training=False)

        # è®¡ç®— MSE é‡å»ºæŸå¤±
        mse_loss = tf.reduce_mean(tf.square(x_batch - y_pred))

        # è®¡ç®— KL Lossï¼ˆTensorFlow è®¡ç®—ï¼Œé˜²æ­¢æº¢å‡ºï¼‰
        kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))

        # è®¡ç®—æ€»æŸå¤±
        loss = mse_loss + kl_loss

        # æ›´æ–°ç´¯åŠ æŸå¤±
        total_loss.assign_add(loss)
        total_reco_loss.assign_add(mse_loss)
        total_kl_loss.assign_add(kl_loss)
        num_batches.assign_add(1)

    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / tf.cast(num_batches, tf.float32)
    avg_reco_loss = total_reco_loss / tf.cast(num_batches, tf.float32)
    avg_kl_loss = total_kl_loss / tf.cast(num_batches, tf.float32)

    print(f"\nğŸ“Š {dataset_type} - Total Loss: {avg_loss:.8f}, MSE Loss: {avg_reco_loss:.8f}, KL Loss: {avg_kl_loss:.8f}")
    
    return avg_loss.numpy(), avg_reco_loss.numpy(), avg_kl_loss.numpy()

# è®¡ç®— **æœ€åä¸€ä¸ª epoch** çš„æŸå¤±
final_train_loss, final_train_reco_loss, final_train_kl_loss = compute_losses(train_dataset, dataset_type="Train")
final_val_loss, final_val_reco_loss, final_val_kl_loss = compute_losses(val_dataset, dataset_type="Validation")
"""